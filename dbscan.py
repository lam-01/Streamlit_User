import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import mlflow
import mlflow.sklearn
from mlflow.tracking import MlflowClient
from sklearn.cluster import KMeans, DBSCAN
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from openml import datasets
import datetime
import os
import plotly.express as px
import plotly.graph_objects as go
from PIL import Image
import io
from sklearn.datasets import fetch_openml

# Set up MLflow
def setup_mlflow():
    mlflow_tracking_uri = "./mlruns"
    if not os.path.exists(mlflow_tracking_uri):
        os.makedirs(mlflow_tracking_uri)
    mlflow.set_tracking_uri(mlflow_tracking_uri)
    return MlflowClient()

# T·∫£i b·ªô d·ªØ li·ªáu MNIST t·ª´ OpenML
@st.cache_data
def load_mnist_data():
    mnist = fetch_openml('mnist_784', version=1, parser='auto')
    X = mnist.data.astype('float32').values
    y = mnist.target.astype('int')
    return X, y

# Preprocess data
@st.cache_data
def preprocess_data(X, sample_size=5000):
    np.random.seed(42)
    indices = np.random.choice(X.shape[0], sample_size, replace=False)
    X_sample = X[indices]
    X_original = X_sample.copy()
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_sample)
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X_scaled)
    return X_sample, X_scaled, X_pca, indices, X_original, pca 

# Th·ª±c hi·ªán ph√¢n c·ª•m K-Means
def run_kmeans(X_scaled, n_clusters=10):
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    return kmeans

# Th·ª±c hi·ªán ph√¢n c·ª•m DBSCAN
def run_dbscan(X_scaled, eps=0.5, min_samples=5):
    dbscan = DBSCAN(eps=eps, min_samples=min_samples)
    dbscan.fit(X_scaled)
    return dbscan

# H√†m log m√¥ h√¨nh v√†o MLFlow
def log_model(model, model_name, params, metrics, cluster_images, experiment_name="MNIST_Clustering_Experiment"):
    mlflow.set_experiment(experiment_name)  # Th√≠ nghi·ªám duy nh·∫•t
    with mlflow.start_run(run_name=model_name) as run:
        mlflow.log_param("model_name", model_name)
        for param_name, param_value in params.items():
            mlflow.log_param(param_name, param_value)
        for metric_name, metric_value in metrics.items():
            mlflow.log_metric(metric_name, metric_value)
        mlflow.sklearn.log_model(model, "model")
        for cluster_id, images in cluster_images.items():
            if images is not None and len(images) > 0:
                for i, img_data in enumerate(images[:5]):
                    fig, ax = plt.subplots(figsize=(3, 3))
                    ax.imshow(img_data.reshape(28, 28), cmap='gray')
                    plt.axis('off')
                    buf = io.BytesIO()
                    plt.savefig(buf, format='png')
                    buf.seek(0)
                    mlflow.log_image(Image.open(buf), f"cluster_{cluster_id}_sample_{i}.png")
                    plt.close(fig)
        return run.info.run_id

# Tr·ª±c quan h√≥a k·∫øt qu·∫£ ph√¢n c·ª•m
def visualize_clusters(X_pca, labels, model_type, centroids=None):
    fig = px.scatter(
        x=X_pca[:, 0], 
        y=X_pca[:, 1], 
        color=[str(label) for label in labels],
        title=f"{model_type} Clustering Results (PCA Visualization)",
        labels={"x": "PCA Component 1", "y": "PCA Component 2"},
        color_discrete_sequence=px.colors.qualitative.G10
    )
    if centroids is not None and model_type == 'K-means':
        pca = PCA(n_components=2)
        pca.fit(X_pca)
        centroids_pca = pca.transform(centroids)
        fig.add_trace(
            go.Scatter(
                x=centroids_pca[:, 0],
                y=centroids_pca[:, 1],
                mode='markers',
                marker=dict(symbol='x', color='black', size=10, line=dict(width=2)),
                name='Centroids'
            )
        )
    return fig

# Nh·∫≠n c√°c v√≠ d·ª• v·ªÅ ch·ªØ s·ªë b·∫±ng c·ª•m
def get_digit_examples_by_cluster(X_original, cluster_labels):
    examples_by_cluster = {}
    unique_labels = np.unique(cluster_labels)
    for label in unique_labels:
        if label == -1:
            continue
        cluster_indices = np.where(cluster_labels == label)[0]
        cluster_samples = X_original[cluster_indices]
        examples_by_cluster[label] = cluster_samples
    return examples_by_cluster

# Danh s√°ch th√≠ nghi·ªám
def list_experiments(client):
    return client.search_experiments()

# Li·ªát k√™ t·∫•t c·∫£ c√°c l·∫ßn ch·∫°y cho m·ªôt th·ª≠ nghi·ªám
def list_runs(client, experiment_id):
    return client.search_runs(experiment_id)

# T√¨m ki·∫øm c√°c m√¥ h√¨nh theo t√™n
def search_models(client, query, experiment_id):
    runs = client.search_runs(
        experiment_id,
        filter_string=f"params.model_name LIKE '%{query}%'"
    )
    return runs

# Nh·∫≠n chi ti·∫øt m√¥ h√¨nh
def get_model_details(client, run_id):
    run = client.get_run(run_id)
    return run

# ·ª®ng d·ª•ng Streamlit ch√≠nh
def main():
    st.title("MNIST Clustering ")
    
    client = setup_mlflow()
    
    tab1, tab2, tab3 = st.tabs(["T·ªïng quan ", "Ph√¢n c·ª•m ", "MLFlow"])

    with tab1:
        try:
            X, y = load_mnist_data()
            st.subheader("üîπ M·ªôt v√†i m·∫´u d·ªØ li·ªáu t·ª´ MNIST")
            if len(X) == 0 or len(y) == 0:
                st.error("D·ªØ li·ªáu MNIST tr·ªëng. Vui l√≤ng ki·ªÉm tra l·∫°i h√†m t·∫£i d·ªØ li·ªáu.")
            else:
                cols = st.columns(5)
                for i, col in enumerate(cols):
                    idx = np.random.randint(0, len(X))
                    with col:
                        fig, ax = plt.subplots(figsize=(3, 3))
                        ax.imshow(X[idx].reshape(28, 28), cmap='gray')
                        ax.set_title(f"Digit: {y[idx]}")
                        ax.axis('off')
                        st.pyplot(fig)
                        plt.close(fig)
        except Exception as e:
            st.error(f"Error loading MNIST data: {e}")
            st.error(f"Chi ti·∫øt l·ªói: {str(e)}")

    with tab2:
        st.header("Run Clustering Algorithms")
        
        try:
            X, y = load_mnist_data()
            st.success(f"B·ªô d·ªØ li·ªáu MNIST ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng v·ªõi {X.shape[0]} m·∫´u.")
            
            sample_size = st.slider("Ch·ªçn c·ª° m·∫´u ƒë·ªÉ ph√¢n c·ª•m", 
                                    min_value=1000, 
                                    max_value=10000, 
                                    value=5000, 
                                    step=1000)
            
            X_sample, X_scaled, X_pca, indices, X_original, pca = preprocess_data(X, sample_size)
            st.success(f"S·ªë l∆∞·ª£ng m·∫´u: {sample_size} m·∫´u.")
            
            # Nh·∫≠p t√™n m√¥ h√¨nh (model_name) thay v√¨ t√™n experiment
            model_name_input = st.text_input("Nh·∫≠p t√™n m√¥ h√¨nh:", "My_Model")
            if not model_name_input:
                model_name_input = "My_Model"
            st.write(f"T√™n m√¥ h√¨nh hi·ªán t·∫°i: {model_name_input}")
            
            selected_tab = st.selectbox("Ch·ªçn thu·∫≠t to√°n ph√¢n c·ª•m", ["K-means", "DBSCAN"])

            if selected_tab == "K-means":
                st.subheader("K-means Clustering")
                n_clusters = st.slider("S·ªë c·ª•m (k)", min_value=5, max_value=20, value=10)
                
                if st.button("Run K-means"):
                    with st.spinner("Ch·∫°y ph√¢n c·ª•m K-Means ..."):
                        kmeans_model = run_kmeans(X_scaled, n_clusters)
                        kmeans_labels = kmeans_model.labels_
                        
                        if len(np.unique(kmeans_labels)) > 1:
                            silhouette = silhouette_score(X_scaled, kmeans_labels)
                            calinski = calinski_harabasz_score(X_scaled, kmeans_labels)
                        else:
                            silhouette = 0
                            calinski = 0
                        st.markdown("C√°c s·ªë li·ªáu ph√¢n c·ª•m", help="""**Silhouette Score** ƒëo l∆∞·ªùng m·ª©c ƒë·ªô t∆∞∆°ng ƒë·ªìng c·ªßa m·ªôt ƒëi·ªÉm v·ªõi c√°c ƒëi·ªÉm trong c√πng m·ªôt c·ª•m so v·ªõi c√°c ƒëi·ªÉm trong c·ª•m kh√°c.
                        \n- Gi√° tr·ªã c·ªßa Silhouette Score n·∫±m trong kho·∫£ng t·ª´ -1 ƒë·∫øn 1:
                        \n +G·∫ßn 1: ƒêi·ªÉm n·∫±m g·∫ßn c√°c ƒëi·ªÉm trong c√πng m·ªôt c·ª•m v√† xa c√°c ƒëi·ªÉm trong c·ª•m kh√°c, cho th·∫•y ph√¢n c·ª•m t·ªët.
                        \n +G·∫ßn 0: ƒêi·ªÉm n·∫±m ·ªü ranh gi·ªõi gi·ªØa hai c·ª•m, cho th·∫•y ph√¢n c·ª•m kh√¥ng r√µ r√†ng.
                        \n +G·∫ßn -1: ƒêi·ªÉm c√≥ th·ªÉ ƒë√£ ƒë∆∞·ª£c ph√¢n c·ª•m sai, n·∫±m g·∫ßn c√°c ƒëi·ªÉm trong c·ª•m kh√°c h∆°n l√† trong c·ª•m c·ªßa n√≥.
                        \n
                        \n **Calinski-Harabasz Score** ƒëo l∆∞·ªùng s·ª± ph√¢n t√°ch gi·ªØa c√°c c·ª•m v√† s·ª± ƒë·ªìng nh·∫•t b√™n trong c√°c c·ª•m.
                        \n- Gi√° tr·ªã c·ªßa Calinski-Harabasz Score c√†ng cao th√¨ ch·∫•t l∆∞·ª£ng ph√¢n c·ª•m c√†ng t·ªët.
                        """)
                        st.write(f"Silhouette Score: {silhouette:.4f}")
                        st.write(f"Calinski-Harabasz Score: {calinski:.4f}")
                        
                        digit_examples = get_digit_examples_by_cluster(X_original, kmeans_labels)
                        
                        params = {
                            "algorithm": "KMeans",
                            "n_clusters": n_clusters,
                            "sample_size": sample_size
                        }
                        metrics = {
                            "silhouette_score": silhouette,
                            "calinski_harabasz_score": calinski
                        }
                        
                        # S·ª≠ d·ª•ng t√™n m√¥ h√¨nh do ng∆∞·ªùi d√πng nh·∫≠p
                        run_id = log_model(kmeans_model, model_name_input, params, metrics, digit_examples)
                        st.success(f"M√¥ h√¨nh K-means ƒë∆∞·ª£c l∆∞u v√†o MLflow v·ªõi run ID: {run_id}")
                        
                        st.subheader("C√°c ch·ªØ s·ªë m·∫´u t·ª´ m·ªói c·ª•m")
                        for cluster_idx in range(min(n_clusters)):
                            if cluster_idx in digit_examples:
                                st.write(f"Cluster {cluster_idx}")
                                cols = st.columns(5)
                                for i, col in enumerate(cols):
                                    if i < len(digit_examples[cluster_idx]):
                                        with col:
                                            fig, ax = plt.subplots(figsize=(2, 2))
                                            ax.imshow(digit_examples[cluster_idx][i].reshape(28, 28), cmap='gray')
                                            ax.axis('off')
                                            st.pyplot(fig)
                                            plt.close(fig)

            elif selected_tab == "DBSCAN":
                st.subheader("Ph√¢n c·ª•m DBSCAN")
                eps = st.slider("Epsilon", min_value=0.1, max_value=10.0, value=5.0, step=0.1, help="""**Epsilon** : B√°n k√≠nh ƒë·ªÉ x√°c ƒë·ªãnh khu v·ª±c l√¢n c·∫≠n c·ªßa m·ªôt ƒëi·ªÉm.
                \n- N·∫øu m·ªôt ƒëi·ªÉm c√≥ ƒë·ªß s·ªë l∆∞·ª£ng h√†ng x√≥m (‚â• min_samples) trong ph·∫°m vi eps, n√≥ s·∫Ω tr·ªü th√†nh core point v√† gi√∫p t·∫°o c·ª•m.
                \n- Gi√° tr·ªã eps c√†ng l·ªõn(6-10), th√¨ c·ª•m c√†ng r·ªông v√† s·ªë l∆∞·ª£ng c·ª•m gi·∫£m xu·ªëng.
                \n- N·∫øu eps qu√° nh·ªè(0.1-2), thu·∫≠t to√°n c√≥ th·ªÉ t·∫°o qu√° nhi·ªÅu c·ª•m nh·ªè ho·∫∑c kh√¥ng t√¨m th·∫•y c·ª•m n√†o.
                 """)
                min_samples = st.slider("Min Samples", min_value=2, max_value=50, value=10, help="""**MinPts** : S·ªë l∆∞·ª£ng ƒëi·ªÉm t·ªëi thi·ªÉu c·∫ßn thi·∫øt ƒë·ªÉ m·ªôt khu v·ª±c ƒë∆∞·ª£c coi l√† ƒë·ªß m·∫≠t ƒë·ªô.
                \n- N·∫øu min_samples nh·ªè(2-5), c√°c c·ª•m c√≥ th·ªÉ d·ªÖ d√†ng h√¨nh th√†nh, ngay c·∫£ v·ªõi d·ªØ li·ªáu nhi·ªÖu.
                \n- N·∫øu min_samples l·ªõn(>30), thu·∫≠t to√°n c√≥ th·ªÉ kh√≥ nh·∫≠n di·ªán c·ª•m nh·ªè v√† c√≥ th·ªÉ ƒë√°nh d·∫•u nhi·ªÅu ƒëi·ªÉm l√† nhi·ªÖu.
                """)
                
                if st.button("Run DBSCAN"):
                    with st.spinner("Ch·∫°y ph√¢n c·ª•m DBSCAN ..."):
                        dbscan_model = run_dbscan(X_scaled, eps, min_samples)
                        dbscan_labels = dbscan_model.labels_
                        
                        n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)
                        if n_clusters > 1:
                            silhouette = silhouette_score(X_scaled, dbscan_labels)
                            calinski = calinski_harabasz_score(X_scaled, dbscan_labels)
                        else:
                            silhouette = 0
                            calinski = 0
                        
                        st.subheader("K·∫øt qu·∫£ ph√¢n c·ª•m")
                        st.write(f"S·ªë l∆∞·ª£ng c·ª•m ƒë∆∞·ª£c t√¨m th·∫•y: {n_clusters}")
                        noise_points = np.sum(dbscan_labels == -1)
                        st.write(f"S·ªë ƒëi·ªÉm nhi·ªÖu: {noise_points} ({noise_points / len(dbscan_labels) * 100:.2f}%)")
                        
                        st.markdown("C√°c s·ªë li·ªáu ph√¢n c·ª•m",help="""**Silhouette Score** ƒëo l∆∞·ªùng m·ª©c ƒë·ªô t∆∞∆°ng ƒë·ªìng c·ªßa m·ªôt ƒëi·ªÉm v·ªõi c√°c ƒëi·ªÉm trong c√πng m·ªôt c·ª•m so v·ªõi c√°c ƒëi·ªÉm trong c·ª•m kh√°c.
                        \n- Gi√° tr·ªã c·ªßa Silhouette Score n·∫±m trong kho·∫£ng t·ª´ -1 ƒë·∫øn 1:
                        \n +G·∫ßn 1: ƒêi·ªÉm n·∫±m g·∫ßn c√°c ƒëi·ªÉm trong c√πng m·ªôt c·ª•m v√† xa c√°c ƒëi·ªÉm trong c·ª•m kh√°c, cho th·∫•y ph√¢n c·ª•m t·ªët.
                        \n +G·∫ßn 0: ƒêi·ªÉm n·∫±m ·ªü ranh gi·ªõi gi·ªØa hai c·ª•m, cho th·∫•y ph√¢n c·ª•m kh√¥ng r√µ r√†ng.
                        \n +G·∫ßn -1: ƒêi·ªÉm c√≥ th·ªÉ ƒë√£ ƒë∆∞·ª£c ph√¢n c·ª•m sai, n·∫±m g·∫ßn c√°c ƒëi·ªÉm trong c·ª•m kh√°c h∆°n l√† trong c·ª•m c·ªßa n√≥.
                        \n
                        \n **Calinski-Harabasz Score** ƒëo l∆∞·ªùng s·ª± ph√¢n t√°ch gi·ªØa c√°c c·ª•m v√† s·ª± ƒë·ªìng nh·∫•t b√™n trong c√°c c·ª•m.
                        \n- Gi√° tr·ªã c·ªßa Calinski-Harabasz Score c√†ng cao th√¨ ch·∫•t l∆∞·ª£ng ph√¢n c·ª•m c√†ng t·ªët.
                        """)
                        st.write(f"Silhouette Score: {silhouette:.4f}")
                        st.write(f"Calinski-Harabasz Score: {calinski:.4f}")
                        
                        digit_examples = get_digit_examples_by_cluster(X_original, dbscan_labels)
                        
                        params = {
                            "algorithm": "DBSCAN",
                            "eps": eps,
                            "min_samples": min_samples,
                            "sample_size": sample_size
                        }
                        metrics = {
                            "silhouette_score": silhouette,
                            "calinski_harabasz_score": calinski,
                            "n_clusters": n_clusters,
                            "noise_percentage": noise_points / len(dbscan_labels) * 100
                        }
                        
                        # S·ª≠ d·ª•ng t√™n m√¥ h√¨nh do ng∆∞·ªùi d√πng nh·∫≠p
                        run_id = log_model(dbscan_model, model_name_input, params, metrics, digit_examples)
                        
                        st.subheader("C√°c ch·ªØ s·ªë m·∫´u t·ª´ m·ªói c·ª•m")
                        unique_labels = sorted(set(dbscan_labels))
                        if -1 in unique_labels:
                            unique_labels.remove(-1)
                            
                        for cluster_idx in unique_labels:
                            if cluster_idx in digit_examples:
                                st.write(f"C·ª•m {cluster_idx}")
                                cols = st.columns(5)
                                for i, col in enumerate(cols):
                                    if i < len(digit_examples[cluster_idx]):
                                        with col:
                                            fig, ax = plt.subplots(figsize=(2, 2))
                                            ax.imshow(digit_examples[cluster_idx][i].reshape(28, 28), cmap='gray')
                                            ax.axis('off')
                                            st.pyplot(fig)
                                            plt.close(fig)
                        
                        if -1 in dbscan_labels:
                            noise_indices = np.where(dbscan_labels == -1)[0]
                            if len(noise_indices) > 0:
                                st.write("ƒêi·ªÉm nhi·ªÖu m·∫´u")
                                cols = st.columns(5)
                                for i, col in enumerate(cols):
                                    if i < min(5, len(noise_indices)):
                                        idx = noise_indices[i]
                                        with col:
                                            fig, ax = plt.subplots(figsize=(2, 2))
                                            ax.imshow(X_original[idx].reshape(28, 28), cmap='gray')
                                            ax.axis('off')
                                            st.pyplot(fig)
                                            plt.close(fig)
        
        except Exception as e:
            st.error(f"Error: {e}")
            st.error(f"Error details: {str(e)}")

    with tab3:
        st.header("MLflow Tracking")
        client = setup_mlflow()
        
        experiments = list_experiments(client)
        if not experiments:
            st.warning("Kh√¥ng t√¨m th·∫•y th√≠ nghi·ªám n√†o trong MLflow. Vui l√≤ng ch·∫°y m·ªôt s·ªë thu·∫≠t to√°n ph√¢n c·ª•m tr∆∞·ªõc!")
        else:
            # Ch·ªâ s·ª≠ d·ª•ng th√≠ nghi·ªám "MNIST_Clustering_Experiment"
            selected_exp = next((exp for exp in experiments if exp.name == "MNIST_Clustering_Experiment"), None)
            if not selected_exp:
                st.warning("Th√≠ nghi·ªám 'MNIST_Clustering_Experiment' ch∆∞a t·ªìn t·∫°i. Ch·∫°y ph√¢n c·ª•m ƒë·ªÉ t·∫°o!")
            else:
                runs = list_runs(client, selected_exp.experiment_id)
                if not runs:
                    st.warning("Kh√¥ng t√¨m th·∫•y run n√†o trong th√≠ nghi·ªám n√†y!")
                else:
                    search_query = st.text_input("T√¨m ki·∫øm theo t√™n m√¥ h√¨nh", "")
                    
                    run_data = []
                    for run in runs:
                        model_name = run.data.params.get("model_name", "Unknown")
                        algorithm = run.data.params.get("algorithm", "Unknown")
                        if algorithm == "KMeans":
                            main_param = f"n_clusters={run.data.params.get('n_clusters', 'N/A')}"
                        elif algorithm == "DBSCAN":
                            main_param = f"eps={run.data.params.get('eps', 'N/A')}, min_samples={run.data.params.get('min_samples', 'N/A')}"
                        else:
                            main_param = "N/A"
                        silhouette = run.data.metrics.get("silhouette_score", "N/A")
                        calinski = run.data.metrics.get("calinski_harabasz_score", "N/A")
                        run_data.append({
                            "Model Name": model_name,
                            "Algorithm": algorithm,
                            "Main Parameters": main_param,
                            "Silhouette Score": silhouette if silhouette != "N/A" else "N/A",
                            "Calinski-Harabasz Score": calinski if calinski != "N/A" else "N/A",
                            "Run ID": run.info.run_id,
                            "Start Time": run.info.start_time
                        })
                    
                    run_df = pd.DataFrame(run_data)
                    run_df["Start Time"] = pd.to_datetime(run_df["Start Time"]).dt.strftime('%Y-%m-%d %H:%M:%S')
                    
                    if search_query:
                        filtered_df = run_df[run_df["Model Name"].str.contains(search_query, case=False, na=False)]
                    else:
                        filtered_df = run_df
                    
                    st.dataframe(filtered_df.drop("Run ID", axis=1), use_container_width=True)
                    
                    selected_model = st.selectbox(
                        "Ch·ªçn m·ªôt m√¥ h√¨nh ƒë·ªÉ xem chi ti·∫øt",
                        options=filtered_df["Model Name"].unique()
                    )
                    
                    if selected_model:
                        selected_run_id = filtered_df[filtered_df["Model Name"] == selected_model]["Run ID"].iloc[0]
                        run_details = get_model_details(client, selected_run_id)
                        
                        st.write("**Parameters:**")
                        params_df = pd.DataFrame(list(run_details.data.params.items()), columns=["Parameter", "Value"])
                        st.dataframe(params_df)
                        
                        st.write("**Metrics:**")
                        metrics_df = pd.DataFrame(list(run_details.data.metrics.items()), columns=["Metric", "Value"])
                        st.dataframe(metrics_df)
                        
                        try:
                            model_uri = f"runs:/{selected_run_id}/model"
                            model = mlflow.sklearn.load_model(model_uri)
                            if run_details.data.params.get("algorithm") == "KMeans":
                                st.write("**Cluster Centers Shape:**", model.cluster_centers_.shape)
                                st.write("**Iterations:**", model.n_iter_)
                                st.subheader("Cluster Centers Visualization")
                                cols = st.columns(5)
                                for i, col in enumerate(cols):
                                    if i < min(5, model.cluster_centers_.shape[0]):
                                        with col:
                                            fig, ax = plt.subplots(figsize=(3, 3))
                                            ax.imshow(model.cluster_centers_[i].reshape(28, 28), cmap='gray')
                                            ax.set_title(f"Cluster {i}")
                                            ax.axis('off')
                                            st.pyplot(fig)
                                            plt.close(fig)
                            elif run_details.data.params.get("algorithm") == "DBSCAN":
                                st.write("**Core Samples:**", len(model.core_sample_indices_))
                        except Exception as e:
                            st.error(f"Kh√¥ng th·ªÉ t·∫£i m√¥ h√¨nh: {e}")

if __name__ == "__main__":
    main()
